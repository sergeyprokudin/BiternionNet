{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten, Activation, Merge, Concatenate, Add\n",
    "from keras import layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from models import vgg\n",
    "from models.cvae import CVAE\n",
    "from utils.angles import deg2bit, bit2deg\n",
    "from utils.losses import mad_loss_tf, cosine_loss_tf, von_mises_loss_tf, maad_from_deg\n",
    "from utils.losses import gaussian_kl_divergence_tf, gaussian_kl_divergence_np\n",
    "from utils.losses  import von_mises_log_likelihood_tf, von_mises_log_likelihood_np\n",
    "from utils.towncentre import load_towncentre\n",
    "from utils.experiements import get_experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TownCentre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, ytr_deg, xval, yval_deg, xte, yte_deg = load_towncentre('data/TownCentre.pkl.gz', canonical_split=True)\n",
    "image_height, image_width = xtr.shape[1], xtr.shape[2]\n",
    "ytr_bit = deg2bit(ytr_deg)\n",
    "yval_bit = deg2bit(yval_deg)\n",
    "yte_bit = deg2bit(yte_deg)\n",
    "\n",
    "image_height, image_width, n_channels = xtr.shape[1:]\n",
    "flatten_x_shape = xtr[0].flatten().shape[0]\n",
    "phi_shape = yte_bit.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "# fig, axs = plt.subplots(1, 10, figsize=(30, 15))\n",
    "# for i in range(0, 10):\n",
    "#     axs[i].imshow(xtr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "\n",
    "$x$ - image,\n",
    "\n",
    "$\\phi$ - head angle,\n",
    "\n",
    "$u$ - hidden variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior network\n",
    "\n",
    "$ p(u|x) \\sim \\mathcal{N}(\\mu_1(x, \\theta), \\sigma_1(x, \\theta)) $\n",
    "\n",
    "#### Encoder network\n",
    "\n",
    "$ q(u|x,\\phi) \\sim \\mathcal{N}(\\mu_2(x, \\theta), \\sigma_2(x, \\theta)) $\n",
    "\n",
    "#### Sample  $u \\sim \\{p(u|x), q(u|x,\\phi) \\}$\n",
    "\n",
    "#### Decoder network\n",
    "\n",
    "$p(\\phi|u,x) \\sim \\mathcal{VM}(\\mu(x,u,\\theta''), \\kappa(x,u,\\theta'')) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_u = 8\n",
    "\n",
    "cvae = CVAE(n_hidden_units=n_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from utils.custom_keras_callbacks import SideModelCheckpoint\n",
    "\n",
    "#proper logs format - 'logs/cvae.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "\n",
    "model_ckpt_callback = keras.callbacks.ModelCheckpoint('logs/cvae.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                                      monitor='val_loss',\n",
    "                                                      mode='min',\n",
    "                                                      save_best_only=True,\n",
    "                                                      verbose=1)\n",
    "\n",
    "save_decoder_callback = SideModelCheckpoint('cvae_decoder', model_to_save=cvae.decoder_model, save_path='logs/cvae_decoder.{epoch:02d}-{val_loss:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6166 samples, validate on 686 samples\n",
      "Epoch 1/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.8690Epoch 00000: val_loss improved from inf to 1.66715, saving model to logs/cvae.00-1.67.hdf5\n",
      "val_loss improved from inf to 1.667148, saving cvae_decoder to logs/cvae_decoder.01-1.67.hdf5\n",
      "6166/6166 [==============================] - 74s - loss: 1.8687 - val_loss: 1.6671\n",
      "Epoch 2/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.5452Epoch 00001: val_loss improved from 1.66715 to 1.53082, saving model to logs/cvae.01-1.53.hdf5\n",
      "val_loss improved from 1.667148 to 1.530816, saving cvae_decoder to logs/cvae_decoder.02-1.53.hdf5\n",
      "6166/6166 [==============================] - 71s - loss: 1.5446 - val_loss: 1.5308\n",
      "Epoch 3/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.4489Epoch 00002: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.4490 - val_loss: 1.5976\n",
      "Epoch 4/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.3650Epoch 00003: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.3656 - val_loss: 1.5908\n",
      "Epoch 5/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.3617Epoch 00004: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.3614 - val_loss: 1.6039\n",
      "Epoch 6/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.3537Epoch 00005: val_loss did not improve\n",
      "6166/6166 [==============================] - 71s - loss: 1.3538 - val_loss: 1.5743\n",
      "Epoch 7/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.3127Epoch 00006: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.3124 - val_loss: 1.5676\n",
      "Epoch 8/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.2628Epoch 00007: val_loss did not improve\n",
      "6166/6166 [==============================] - 71s - loss: 1.2626 - val_loss: 1.5534\n",
      "Epoch 9/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.2282Epoch 00008: val_loss did not improve\n",
      "6166/6166 [==============================] - 71s - loss: 1.2280 - val_loss: 1.7210\n",
      "Epoch 10/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.2104Epoch 00009: val_loss improved from 1.53082 to 1.43117, saving model to logs/cvae.09-1.43.hdf5\n",
      "val_loss improved from 1.530816 to 1.431167, saving cvae_decoder to logs/cvae_decoder.10-1.43.hdf5\n",
      "6166/6166 [==============================] - 70s - loss: 1.2106 - val_loss: 1.4312\n",
      "Epoch 11/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.1987Epoch 00010: val_loss improved from 1.43117 to 1.40504, saving model to logs/cvae.10-1.41.hdf5\n",
      "val_loss improved from 1.431167 to 1.405038, saving cvae_decoder to logs/cvae_decoder.11-1.41.hdf5\n",
      "6166/6166 [==============================] - 71s - loss: 1.1987 - val_loss: 1.4050\n",
      "Epoch 12/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.1721Epoch 00011: val_loss improved from 1.40504 to 1.38037, saving model to logs/cvae.11-1.38.hdf5\n",
      "val_loss improved from 1.405038 to 1.380366, saving cvae_decoder to logs/cvae_decoder.12-1.38.hdf5\n",
      "6166/6166 [==============================] - 72s - loss: 1.1721 - val_loss: 1.3804\n",
      "Epoch 13/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.1474Epoch 00012: val_loss improved from 1.38037 to 1.34913, saving model to logs/cvae.12-1.35.hdf5\n",
      "val_loss improved from 1.380366 to 1.349127, saving cvae_decoder to logs/cvae_decoder.13-1.35.hdf5\n",
      "6166/6166 [==============================] - 71s - loss: 1.1479 - val_loss: 1.3491\n",
      "Epoch 14/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.1050Epoch 00013: val_loss did not improve\n",
      "6166/6166 [==============================] - 71s - loss: 1.1049 - val_loss: 1.3948\n",
      "Epoch 15/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.0697Epoch 00014: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.0703 - val_loss: 1.3856\n",
      "Epoch 16/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.0438Epoch 00015: val_loss improved from 1.34913 to 1.25842, saving model to logs/cvae.15-1.26.hdf5\n",
      "val_loss improved from 1.349127 to 1.258421, saving cvae_decoder to logs/cvae_decoder.16-1.26.hdf5\n",
      "6166/6166 [==============================] - 70s - loss: 1.0442 - val_loss: 1.2584\n",
      "Epoch 17/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 1.0290Epoch 00016: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 1.0293 - val_loss: 1.2713\n",
      "Epoch 18/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 0.9895Epoch 00017: val_loss did not improve\n",
      "6166/6166 [==============================] - 71s - loss: 0.9895 - val_loss: 1.2796\n",
      "Epoch 19/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 0.9543Epoch 00018: val_loss did not improve\n",
      "6166/6166 [==============================] - 70s - loss: 0.9546 - val_loss: 1.3374\n",
      "Epoch 20/20\n",
      "6160/6166 [============================>.] - ETA: 0s - loss: 0.9122Epoch 00019: val_loss improved from 1.25842 to 1.25230, saving model to logs/cvae.19-1.25.hdf5\n",
      "val_loss improved from 1.258421 to 1.252302, saving cvae_decoder to logs/cvae_decoder.20-1.25.hdf5\n",
      "6166/6166 [==============================] - 71s - loss: 0.9117 - val_loss: 1.2523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120a9bbe0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.full_model.fit([xtr, ytr_bit], [ytr_bit], batch_size=10, epochs=20, validation_split=0.1,\n",
    "                   callbacks=[model_ckpt_callback, save_decoder_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions using decoder part\n",
    "\n",
    "$ \\phi_i = \\mu(x_i,u_i,\\theta'') $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAAD error (test) : 27.595090 ± 31.254913\n",
      "kappa (test) : 4.427681 ± 2.550267\n",
      "ELBO (test) : -0.895809 ± 0.040572 SEM\n",
      "KL(encoder|prior) (test) : -0.000172 ± 0.000022 SEM\n",
      "log-likelihood (test) : -0.893529±0.039949SEM\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "n_samples = xte.shape[0]\n",
    "#ute = np.random.normal(0,1, [n_samples,n_u])\n",
    "\n",
    "#yte_cvae_preds = cvae.full_model.predict([xte, yte_bit])\n",
    "\n",
    "cvae_preds = cvae.full_model.predict([xte, yte_bit])\n",
    "elbo_te, ll_te, kl_te = cvae_elbo_np(yte_bit, cvae_preds)\n",
    "\n",
    "yte_preds = cvae.decoder_model.predict(xte)\n",
    "yte_preds_bit = yte_preds[:,0:2]\n",
    "kappa_preds_te = yte_preds[:,2:]\n",
    "\n",
    "yte_preds_deg = bit2deg(yte_preds_bit)\n",
    "\n",
    "loss_te = maad_from_deg(yte_preds_deg, yte_deg)\n",
    "mean_loss_te = np.mean(loss_te)\n",
    "std_loss_te = np.std(loss_te)\n",
    "\n",
    "print(\"MAAD error (test) : %f ± %f\" % (mean_loss_te, std_loss_te))\n",
    "\n",
    "#kappa_preds_te = np.ones([xte.shape[0], 1]) \n",
    "\n",
    "print(\"kappa (test) : %f ± %f\" % (np.mean(kappa_preds_te), np.std(kappa_preds_te)))\n",
    "\n",
    "log_likelihood_loss_te = von_mises_log_likelihood_np(yte_bit, yte_preds_bit, kappa_preds_te,\n",
    "                                                     input_type='biternion')\n",
    "\n",
    "\n",
    "print(\"ELBO (test) : %f ± %f SEM\" % (np.mean(-elbo_te), sem(-elbo_te)))\n",
    "# print(\"log-likelihood (test) : %f ± %f SEM\" % (np.mean(-ll_te), sem(-ll_te)))\n",
    "print(\"KL(encoder|prior) (test) : %f ± %f SEM\" % (np.mean(-kl_te), sem(-kl_te)))\n",
    "\n",
    "print(\"log-likelihood (test) : %f±%fSEM\" % (np.mean(log_likelihood_loss_te), sem(log_likelihood_loss_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAAD error (train) : 23.851883 ± 28.897305\n",
      "kappa (train) : 4.431825 ± 2.674264\n",
      "ELBO (train) : -0.701403 ± 0.009008 SEM\n",
      "KL(encoder|prior) (train) : -0.000169 ± 0.000008 SEM\n",
      "log-likelihood (train) : -0.701012±0.009037SEM\n"
     ]
    }
   ],
   "source": [
    "n_samples = xtr.shape[0]\n",
    "#utr = np.random.normal(0,1, [n_samples,n_u])\n",
    "\n",
    "#ytr_cvae_preds = cvae.full_model.predict([xtr, ytr_bit])\n",
    "\n",
    "cvae_preds = cvae.full_model.predict([xtr, ytr_bit])\n",
    "elbo_tr, ll_tr, kl_tr = cvae_elbo_np(ytr_bit, cvae_preds)\n",
    "\n",
    "ytr_preds = cvae.decoder_model.predict(xtr)\n",
    "ytr_preds_bit = ytr_preds[:,0:2]\n",
    "kappa_preds_tr = ytr_preds[:,2:]\n",
    "\n",
    "ytr_preds_deg = bit2deg(ytr_preds_bit)\n",
    "\n",
    "loss_tr = maad_from_deg(ytr_preds_deg, ytr_deg)\n",
    "mean_loss_tr = np.mean(loss_tr)\n",
    "std_loss_tr = np.std(loss_tr)\n",
    "\n",
    "print(\"MAAD error (train) : %f ± %f\" % (mean_loss_tr, std_loss_tr))\n",
    "\n",
    "#kappa_preds_tr = np.ones([xtr.shape[0], 1]) \n",
    "\n",
    "print(\"kappa (train) : %f ± %f\" % (np.mean(kappa_preds_tr), np.std(kappa_preds_tr)))\n",
    "\n",
    "log_likelihood_loss_tr = von_mises_log_likelihood_np(ytr_bit, ytr_preds_bit, kappa_preds_tr,\n",
    "                                                     input_type='biternion')\n",
    "\n",
    "\n",
    "\n",
    "print(\"ELBO (train) : %f ± %f SEM\" % (np.mean(-elbo_tr), sem(-elbo_tr)))\n",
    "# print(\"log-likelihood (train) : %f ± %f SEM\" % (np.mean(-ll_tr), sem(-ll_tr)))\n",
    "print(\"KL(encoder|prior) (train) : %f ± %f SEM\" % (np.mean(-kl_tr), sem(-kl_tr)))\n",
    "\n",
    "print(\"log-likelihood (train) : %f±%fSEM\" % (np.mean(log_likelihood_loss_tr), sem(log_likelihood_loss_tr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
