{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb961520-81e7-da2c-5fa7-852f98749f3e",
    "_uuid": "9ee4de01466cfdf97092e3f5a7819e0a4170806d"
   },
   "source": [
    "# Introduction\n",
    "## Overview\n",
    "The notebook applies the [SimGAN](https://arxiv.org/pdf/1612.07828v1.pdf) network architecture to the problem of generating realistic images of eyes by using real images to augment the simulated data. We use Keras with a Tensorflow-backend to accomplish the adversarial training. \n",
    "### Note \n",
    "The model is quite small and the training is tuned down substantially since the limits of Kaggle Kernels and the lack of GPU makes the training very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "bb0b3648-a55b-e9dd-7396-97fbedf4a3c0",
    "_uuid": "e49726407029f64340ac6677f482b346308e88cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d395fcbf-7846-00df-e949-728c55e5528b",
    "_uuid": "1c8ff0102595a39f43626b8f8bdc633259d19a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-version 1.1.0 keras-version 2.0.8\n"
     ]
    }
   ],
   "source": [
    "print('tf-version',tf.__version__, 'keras-version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de81853c-aaff-d745-0f05-57c6d89a1038",
    "_uuid": "e2c2fcb576a1df719f2a8d5687c4c5500a9baa44"
   },
   "source": [
    "## Loading Data\n",
    "Here we setup the paths and load the data from the hdf5 files since there would otherwise be too many individual jpg/png images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c4d71194-04e2-49f0-ad43-d4297fccf7b2",
    "_uuid": "a82f7a374083a0649043d386707fa4a5c2a84846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'look_vec', 'path']\n",
      "Synthetic images found: 50000\n",
      "image ..\\..\\..\\..\\Downloads\\UnityEyes_Windows\\UnityEyes_Windows\\imgs\\1.jpg shape: (35, 55)\n",
      "['image']\n",
      "Real Images found: 34100\n",
      "image ..\\data\\MPIIGaze_Dataset\\000055ed-890a-411f-be6e-b8453b495389.png shape: (35, 55)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.dirname(os.path.abspath('.'))\n",
    "data_dir = \"../data/eye-gaze/\" #os.path.join('..', 'input')\n",
    "cache_dir = '.'\n",
    "\n",
    "# load the data file and extract dimensions\n",
    "with h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "    assert 'path' in t_file, \"Paths are missing\"\n",
    "    print('Synthetic images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "with h5py.File(os.path.join(data_dir,'real_gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Real Images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    real_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "    \n",
    "#\n",
    "# training params\n",
    "#\n",
    "\n",
    "nb_steps = 20 # originally 10000, but this makes the kernel time out\n",
    "batch_size = 49\n",
    "k_d = 1  # number of discriminator updates per step\n",
    "k_g = 2  # number of generative network updates per step\n",
    "log_interval = 100\n",
    "pre_steps = 15 # for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mpigaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24212a20394d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_image_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.imshow(np.squeeze(real_image_stack[23]), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'look_vec', 'path']\n",
      "Synthetic images found: 50000\n",
      "image ..\\..\\..\\..\\Downloads\\UnityEyes_Windows\\UnityEyes_Windows\\imgs\\1.jpg shape: (35, 55)\n"
     ]
    }
   ],
   "source": [
    "t_file = h5py.File(os.path.join(data_dir,'gaze.h5'),'r')\n",
    "print(list(t_file.keys()))\n",
    "assert 'image' in t_file, \"Images are missing\"\n",
    "assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "assert 'path' in t_file, \"Paths are missing\"\n",
    "print('Synthetic images found:',len(t_file['image']))\n",
    "for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "    print('image',ikey,'shape:',ival.shape)\n",
    "    img_height, img_width = ival.shape\n",
    "    img_channels = 1\n",
    "syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_group_key = list(t_file.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_vec = np.asarray(t_file['look_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6431, -0.1524, -0.7504,  0.    ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4881321d-64f8-fe2a-d8bb-1c74500754a4",
    "_uuid": "6f7660bc9f395dc96077bcc47b3ce73c18749d83"
   },
   "source": [
    "## Utility Functions\n",
    "These functions make it a bit easier to keep track of how the training is going and make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "da7514ef-2b69-8945-79da-fb0583dd36ea",
    "_uuid": "6e62d470969b51a88b7dd0a737d5cb29a53e2ec7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Module to plot a batch of images along w/ their corresponding label(s)/annotations and save the plot to disc.\n",
    "\n",
    "Use cases:\n",
    "Plot images along w/ their corresponding ground-truth label & model predicted label,\n",
    "Plot images generated by a GAN along w/ any annotations used to generate these images,\n",
    "Plot synthetic, generated, refined, and real images and see how they compare as training progresses in a GAN,\n",
    "etc...\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from itertools import groupby\n",
    "from skimage.util.montage import montage2d\n",
    "def plot_batch(image_batch, figure_path, label_batch=None):\n",
    "    \n",
    "    all_groups = {label: montage2d(np.stack([img[:,:,0] for img, lab in img_lab_list],0)) \n",
    "                  for label, img_lab_list in groupby(zip(image_batch, label_batch), lambda x: x[1])}\n",
    "    fig, c_axs = plt.subplots(1,len(all_groups), figsize=(len(all_groups)*4, 8), dpi = 600)\n",
    "    for c_ax, (c_label, c_mtg) in zip(c_axs, all_groups.items()):\n",
    "        c_ax.imshow(c_mtg, cmap='bone')\n",
    "        c_ax.set_title(c_label)\n",
    "        c_ax.axis('off')\n",
    "    fig.savefig(os.path.join(figure_path))\n",
    "    plt.close()\n",
    "\"\"\"\n",
    "Module implementing the image history buffer described in `2.3. Updating Discriminator using a History of\n",
    "Refined Images` of https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "\"\"\"\n",
    "class ImageHistoryBuffer(object):\n",
    "    def __init__(self, shape, max_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the class's state.\n",
    "\n",
    "        :param shape: Shape of the data to be stored in the image history buffer\n",
    "                      (i.e. (0, img_height, img_width, img_channels)).\n",
    "        :param max_size: Maximum number of images that can be stored in the image history buffer.\n",
    "        :param batch_size: Batch size used to train GAN.\n",
    "        \"\"\"\n",
    "        self.image_history_buffer = np.zeros(shape=shape)\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_to_image_history_buffer(self, images, nb_to_add=None):\n",
    "        \"\"\"\n",
    "        To be called during training of GAN. By default add batch_size // 2 images to the image history buffer each\n",
    "        time the generator generates a new batch of images.\n",
    "\n",
    "        :param images: Array of images (usually a batch) to be added to the image history buffer.\n",
    "        :param nb_to_add: The number of images from `images` to add to the image history buffer\n",
    "                          (batch_size / 2 by default).\n",
    "        \"\"\"\n",
    "        if not nb_to_add:\n",
    "            nb_to_add = self.batch_size // 2\n",
    "\n",
    "        if len(self.image_history_buffer) < self.max_size:\n",
    "            np.append(self.image_history_buffer, images[:nb_to_add], axis=0)\n",
    "        elif len(self.image_history_buffer) == self.max_size:\n",
    "            self.image_history_buffer[:nb_to_add] = images[:nb_to_add]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        np.random.shuffle(self.image_history_buffer)\n",
    "\n",
    "    def get_from_image_history_buffer(self, nb_to_get=None):\n",
    "        \"\"\"\n",
    "        Get a random sample of images from the history buffer.\n",
    "\n",
    "        :param nb_to_get: Number of images to get from the image history buffer (batch_size / 2 by default).\n",
    "        :return: A random sample of `nb_to_get` images from the image history buffer, or an empty np array if the image\n",
    "                 history buffer is empty.\n",
    "        \"\"\"\n",
    "        if not nb_to_get:\n",
    "            nb_to_get = self.batch_size // 2\n",
    "\n",
    "        try:\n",
    "            return self.image_history_buffer[:nb_to_get]\n",
    "        except IndexError:\n",
    "            return np.zeros(shape=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8e42268-05b1-0e33-9fe1-f02e0a6d234e",
    "_uuid": "983243bdc12af55b03d6c2c8e8bd46658f4ea667"
   },
   "source": [
    "# Network Architectures\n",
    "Here we define the two primary networks\n",
    "\n",
    " - refiner network\n",
    " - discriminator network\n",
    "\n",
    "They work against each other to constantly improve the results of the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "701fd9e1-f970-8420-504b-7c057e0173a2",
    "_uuid": "7f1853ca3c5ec70998751c75a5a98fc957afc227",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refiner_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The refiner network, Rθ, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n",
    "    than holistically modifying the image content, preserving the global structure and annotations.\n",
    "\n",
    "    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n",
    "    :return: Output tensor that corresponds to a refined synthetic image.\n",
    "    \"\"\"\n",
    "    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "        \"\"\"\n",
    "        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n",
    "        each with `nb_features` feature maps.\n",
    "\n",
    "        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "        :param input_features: Input tensor to ResNet block.\n",
    "        :return: Output tensor from ResNet block.\n",
    "        \"\"\"\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(input_features)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(y)\n",
    "\n",
    "        y = layers.merge([input_features, y], mode='sum')\n",
    "        return layers.Activation('relu')(y)\n",
    "\n",
    "    # an input image of size w × h is convolved with 3 × 3 filters that output 64 feature maps\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_image_tensor)\n",
    "\n",
    "    # the output is passed through 4 ResNet blocks\n",
    "    for _ in range(4):\n",
    "        x = resnet_block(x)\n",
    "\n",
    "    # the output of the last ResNet block is passed to a 1 × 1 convolutional layer producing 1 feature map\n",
    "    # corresponding to the refined synthetic image\n",
    "    return layers.Convolution2D(img_channels, 1, 1, border_mode='same', activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "35420452-e116-f91a-a386-c2f8f290effe",
    "_uuid": "a9e232170bffab6f0576826aa6f0179826222733",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The discriminator network, Dφ, contains 5 convolution layers and 2 max-pooling layers.\n",
    "\n",
    "    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n",
    "    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n",
    "    \"\"\"\n",
    "    x = layers.Convolution2D(96, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(input_image_tensor)\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), border_mode='same', strides=(1, 1))(x)\n",
    "    x = layers.Convolution2D(32, 3, 3, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(32, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(2, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "\n",
    "    # here one feature map corresponds to `is_real` and the other to `is_refined`,\n",
    "    # and the custom loss function is then `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "    return layers.Reshape((-1, 2))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "365c0d27-8e84-1561-2434-3bde90c31534",
    "_uuid": "e6b69d778069ad4334a935bc294f8b15bde38692"
   },
   "source": [
    "## Combining Models\n",
    "Adversarial training of refiner network Rθ and discriminator network Dφ and combining them into single loss-functions and models that can be trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "fe1d7259-a48d-bb66-e471-635ac4958124",
    "_uuid": "4595688e2fe01b6d1759c5ebb61b809328e743a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"tanh\", padding=\"same\")`\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  \n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"refiner\", inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"discriminator\", inputs=Tensor(\"in..., outputs=Tensor(\"re...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 35, 55, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 55, 64)   640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 55, 64)   36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 55, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 55, 64)   36928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_5 (Merge)                 (None, 35, 55, 64)   0           conv2d_11[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 55, 64)   0           merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 55, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 55, 64)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 55, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_6 (Merge)                 (None, 35, 55, 64)   0           activation_10[0][0]              \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 55, 64)   0           merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 55, 64)   36928       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 55, 64)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 55, 64)   36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_7 (Merge)                 (None, 35, 55, 64)   0           activation_12[0][0]              \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 55, 64)   0           merge_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 55, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 55, 64)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 55, 64)   36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_8 (Merge)                 (None, 35, 55, 64)   0           activation_14[0][0]              \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 55, 64)   0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 55, 1)    65          activation_16[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 296,129\n",
      "Trainable params: 296,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 18, 28, 96)        960       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 9, 14, 64)         55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 9, 14, 32)         18464     \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 9, 14, 32)         1056      \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 9, 14, 2)          66        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 126, 2)            0         \n",
      "=================================================================\n",
      "Total params: 75,906\n",
      "Trainable params: 75,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "refiner (Model)              (None, 35, 55, 1)         296129    \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 126, 2)            75906     \n",
      "=================================================================\n",
      "Total params: 372,035\n",
      "Trainable params: 372,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergeyprokudin/boiler_room/py_env/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"combined\", inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# define model input and output tensors\n",
    "#\n",
    "\n",
    "synthetic_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "refined_image_tensor = refiner_network(synthetic_image_tensor)\n",
    "\n",
    "refined_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "discriminator_output = discriminator_network(refined_or_real_image_tensor)\n",
    "\n",
    "#\n",
    "# define models\n",
    "#\n",
    "\n",
    "refiner_model = models.Model(input=synthetic_image_tensor, output=refined_image_tensor, name='refiner')\n",
    "discriminator_model = models.Model(input=refined_or_real_image_tensor, output=discriminator_output,\n",
    "                                   name='discriminator')\n",
    "\n",
    "# combined must output the refined image along w/ the disc's classification of it for the refiner's self-reg loss\n",
    "refiner_model_output = refiner_model(synthetic_image_tensor)\n",
    "combined_output = discriminator_model(refiner_model_output)\n",
    "combined_model = models.Model(input=synthetic_image_tensor, output=[refiner_model_output, combined_output],\n",
    "                              name='combined')\n",
    "\n",
    "discriminator_model_output_shape = discriminator_model.output_shape\n",
    "\n",
    "print(refiner_model.summary())\n",
    "print(discriminator_model.summary())\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33ede9dc-63dc-e7eb-cabf-39ac01fcd5f4",
    "_uuid": "8c03cbc4f25c5c50ee94517499b5ae5dc5a2ffc9"
   },
   "source": [
    "## Visualization\n",
    "Here we show the network architectures visually with shapes (doesn't yet work on kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "44bc1cfc-a7f4-1aa4-432f-bef7473ff3a9",
    "_uuid": "f692c6e34509cfe7c71379d721ee9dce74ec1ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running the patched version of keras/pydot!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "# Define model\n",
    "try:\n",
    "    model_to_dot(refiner_model, show_shapes=True).write_svg('refiner_model.svg')\n",
    "    SVG('refiner_model.svg')\n",
    "except ImportError:\n",
    "    print('Not running the patched version of keras/pydot!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "c2821044-015b-5b2e-4cf7-b22195158767",
    "_uuid": "ea56fe046be73e6de5b3879972532d774649c1d7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_to_dot(discriminator_model, show_shapes=True).write_svg('discriminator_model.svg')\n",
    "    SVG('discriminator_model.svg')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "e7446d98-534f-1094-ee09-245f3f7d28f7",
    "_uuid": "d1881387ff69223a4c537f28235193874e0d8f14",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# define custom l1 loss function for the refiner\n",
    "#\n",
    "\n",
    "def self_regularization_loss(y_true, y_pred):\n",
    "    delta = 0.0001  # FIXME: need to figure out an appropriate value for this\n",
    "    return tf.multiply(delta, tf.reduce_sum(tf.abs(y_pred - y_true)))\n",
    "\n",
    "#\n",
    "# define custom local adversarial loss (softmax for each image section) for the discriminator\n",
    "# the adversarial loss function is the sum of the cross-entropy losses over the local patches\n",
    "#\n",
    "\n",
    "def local_adversarial_loss(y_true, y_pred):\n",
    "    # y_true and y_pred have shape (batch_size, # of local patches, 2), but really we just want to average over\n",
    "    # the local patches and batch size so we can reshape to (batch_size * # of local patches, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 2))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c70b7cb-dd4a-9a46-15f3-02d78fdc53f7",
    "_uuid": "93d7237b52823214a73a6d981563a7e62bb0ea18"
   },
   "source": [
    "# Compile Models\n",
    "Here we combine the models and compile them with the loss functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "3faaf71b-0cfc-59e8-4b37-13ac79d61cfc",
    "_uuid": "7a6c52e8635f4a68384879a4af895dd4889696ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-3)\n",
    "\n",
    "refiner_model.compile(optimizer=sgd, loss=self_regularization_loss)\n",
    "discriminator_model.compile(optimizer=sgd, loss=local_adversarial_loss)\n",
    "discriminator_model.trainable = False\n",
    "combined_model.compile(optimizer=sgd, loss=[self_regularization_loss, local_adversarial_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "d50c46c7-94cf-ba72-ab47-f9dd0a8f36c3",
    "_uuid": "7334d5efa81b8d418452b93d15c177c9bc650218",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "refiner_model_path = None\n",
    "discriminator_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb536531-09b4-d9b7-a23e-f3a82517939f",
    "_uuid": "13b9dfbd7509e77322baa8e16ee9bf559960f635"
   },
   "source": [
    "# Data Generators\n",
    "Here we setup the pipeline to feed new images to both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "ba8320cb-5aef-f682-0e95-48fd2a319ea5",
    "_uuid": "1b6ea13c66bb195dc1142b1c38a499a62ca9e2a5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator(\n",
    "    preprocessing_function=applications.xception.preprocess_input,\n",
    "    data_format='channels_last')\n",
    "\n",
    "flow_from_directory_params = {'target_size': (img_height, img_width),\n",
    "                              'color_mode': 'grayscale' if img_channels == 1 else 'rgb',\n",
    "                              'class_mode': None,\n",
    "                              'batch_size': batch_size}\n",
    "flow_params = {'batch_size': batch_size}\n",
    "\n",
    "synthetic_generator = datagen.flow(\n",
    "    x = syn_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "real_generator = datagen.flow(\n",
    "    x = real_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "def get_image_batch(generator):\n",
    "    \"\"\"keras generators may generate an incomplete batch for the last batch\"\"\"\n",
    "    img_batch = generator.next()\n",
    "    if len(img_batch) != batch_size:\n",
    "        img_batch = generator.next()\n",
    "\n",
    "    assert len(img_batch) == batch_size\n",
    "\n",
    "    return img_batch\n",
    "\n",
    "# the target labels for the cross-entropy loss layer are 0 for every yj (real) and 1 for every xi (refined)\n",
    "y_real = np.array([[[1.0, 0.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "y_refined = np.array([[[0.0, 1.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "assert y_real.shape == (batch_size, discriminator_model_output_shape[1], 2)\n",
    "batch_out = get_image_batch(synthetic_generator)\n",
    "assert batch_out.shape == (batch_size, img_height, img_width, img_channels), \"Image Dimensions do not match, {}!={}\".format(batch_out.shape, (batch_size, img_height, img_width, img_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3670cae4-d254-3648-4a86-ac7e1e585ff2",
    "_uuid": "6114a37b6f104bd0aee0cff028989813619c6cb6"
   },
   "source": [
    "# Pretraining\n",
    "Here we pretraining the models before we start the full-blown training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "22ceec0a-0697-d1ce-05b5-0cb68851edbf",
    "_uuid": "e68567de83b7ca236467f8f04be5e72c206b2383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training the refiner network...\n",
      "Saving batch of refined images during pre-training at step: 0.\n",
      "Refiner model self regularization loss: [ 0.03865056].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-323d8bb9d7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msynthetic_image_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthetic_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefiner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthetic_image_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynthetic_image_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# log every `log_interval` steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/boiler_room/py_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not refiner_model_path:\n",
    "    # we first train the Rθ network with just self-regularization loss for 1,000 steps\n",
    "    print('pre-training the refiner network...')\n",
    "    gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    for i in range(pre_steps):\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        gen_loss = np.add(refiner_model.train_on_batch(synthetic_image_batch, synthetic_image_batch), gen_loss)\n",
    "\n",
    "        # log every `log_interval` steps\n",
    "        if not i % log_interval:\n",
    "            figure_name = 'refined_image_batch_pre_train_step_{}.png'.format(i)\n",
    "            print('Saving batch of refined images during pre-training at step: {}.'.format(i))\n",
    "\n",
    "            synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "            plot_batch(\n",
    "                np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "                os.path.join(cache_dir, figure_name),\n",
    "                label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "            print('Refiner model self regularization loss: {}.'.format(gen_loss / log_interval))\n",
    "            gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    refiner_model.save(os.path.join(cache_dir, 'refiner_model_pre_trained.h5'))\n",
    "else:\n",
    "    refiner_model.load_weights(refiner_model_path)\n",
    "\n",
    "if not discriminator_model_path:\n",
    "    # and Dφ for 200 steps (one mini-batch for refined images, another for real)\n",
    "    print('pre-training the discriminator network...')\n",
    "    disc_loss = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "    for _ in range(pre_steps):\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss)\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined), disc_loss)\n",
    "\n",
    "    discriminator_model.save(os.path.join(cache_dir, 'discriminator_model_pre_trained.h5'))\n",
    "\n",
    "    # hard-coded for now\n",
    "    print('Discriminator model loss: {}.'.format(disc_loss / (100 * 2)))\n",
    "else:\n",
    "    discriminator_model.load_weights(discriminator_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f7acaeb-5a0e-d418-b855-b3e85b7854c0",
    "_uuid": "0f7d2c7c2fde3552060dcd60629b0383c27c28bb"
   },
   "source": [
    "# Full Training\n",
    "Here we run the full training of the model (normally for thousands of iterations, but here just a few). The images are saved in the output directory for examining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d7fb313-ec78-659b-0510-b6a235400294",
    "_uuid": "21e558b6fd859e8aedd56cf30949a7f2ac3d5b50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: what is an appropriate size for the image history buffer?\n",
    "image_history_buffer = ImageHistoryBuffer((0, img_height, img_width, img_channels), batch_size * 100, batch_size)\n",
    "\n",
    "combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "# see Algorithm 1 in https://arxiv.org/pdf/1612.07828v1.pdf\n",
    "for i in range(nb_steps):\n",
    "    print('Step: {} of {}.'.format(i, nb_steps))\n",
    "\n",
    "    # train the refiner\n",
    "    for _ in range(k_g * 2):\n",
    "        # sample a mini-batch of synthetic images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "\n",
    "        # update θ by taking an SGD step on mini-batch loss LR(θ)\n",
    "        combined_loss = np.add(combined_model.train_on_batch(synthetic_image_batch,\n",
    "                                                             [synthetic_image_batch, y_real]), combined_loss)\n",
    "\n",
    "    for _ in range(k_d):\n",
    "        # sample a mini-batch of synthetic and real images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "\n",
    "        # refine the synthetic images w/ the current refiner\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "\n",
    "        # use a history of refined images\n",
    "        half_batch_from_image_history = image_history_buffer.get_from_image_history_buffer()\n",
    "        image_history_buffer.add_to_image_history_buffer(refined_image_batch)\n",
    "\n",
    "        if len(half_batch_from_image_history):\n",
    "            refined_image_batch[:batch_size // 2] = half_batch_from_image_history\n",
    "\n",
    "        # update φ by taking an SGD step on mini-batch loss LD(φ)\n",
    "        disc_loss_real = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss_real)\n",
    "        disc_loss_refined = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined),\n",
    "                                   disc_loss_refined)\n",
    "\n",
    "    if not i % log_interval:\n",
    "        # plot batch of refined images w/ current refiner\n",
    "        figure_name = 'refined_image_batch_step_{}.png'.format(i)\n",
    "        print('Saving batch of refined images at adversarial step: {}.'.format(i))\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        plot_batch(\n",
    "            np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "            os.path.join(cache_dir, figure_name),\n",
    "            label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "        # log loss summary\n",
    "        print('Refiner model loss: {}.'.format(combined_loss / (log_interval * k_g * 2)))\n",
    "        print('Discriminator model loss real: {}.'.format(disc_loss_real / (log_interval * k_d * 2)))\n",
    "        print('Discriminator model loss refined: {}.'.format(disc_loss_refined / (log_interval * k_d * 2)))\n",
    "\n",
    "        combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "        disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "        # save model checkpoints\n",
    "        model_checkpoint_base_name = os.path.join(cache_dir, '{}_model_step_{}.h5')\n",
    "        refiner_model.save(model_checkpoint_base_name.format('refiner', i))\n",
    "        discriminator_model.save(model_checkpoint_base_name.format('discriminator', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0784e5cb-964e-eb52-9739-a3e7591f7375",
    "_uuid": "e7070ccc18e5efd7a9297db6fd62af22f0f29c95",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
