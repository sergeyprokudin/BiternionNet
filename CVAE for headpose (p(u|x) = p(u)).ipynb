{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten, Activation, Merge, Concatenate, Add\n",
    "from keras import layers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from models import vgg\n",
    "from utils.angles import deg2bit, bit2deg\n",
    "from utils.losses import mad_loss_tf, cosine_loss_tf, von_mises_loss_tf, maad_from_deg\n",
    "from utils.losses import gaussian_kl_divergence_tf, gaussian_kl_divergence_np\n",
    "from utils.losses  import von_mises_log_likelihood_tf, von_mises_log_likelihood_np\n",
    "from utils.towncentre import load_towncentre\n",
    "from utils.experiements import get_experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TownCentre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtr, ytr_deg, xte, yte_deg = load_towncentre('data/TownCentre.pkl.gz', canonical_split=True)\n",
    "image_height, image_width = xtr.shape[1], xtr.shape[2]\n",
    "ytr_bit = deg2bit(ytr_deg)\n",
    "yte_bit = deg2bit(yte_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 10, figsize=(30, 15))\n",
    "# for i in range(0, 10):\n",
    "#     axs[i].imshow(xtr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_height, image_width, n_channels = xtr.shape[1:]\n",
    "flatten_x_shape = xtr[0].flatten().shape[0]\n",
    "phi_shape = yte_bit.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "\n",
    "$x$ - image,\n",
    "\n",
    "$\\phi$ - head angle,\n",
    "\n",
    "$u$ - hidden variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior network\n",
    "\n",
    "$ p(u|x) = p(u) \\sim \\mathcal{N}(0, 1) $\n",
    "\n",
    "#### Encoder network\n",
    "\n",
    "$ q(u|x,\\phi) \\sim \\mathcal{N}(\\mu_2(x, \\theta), \\sigma_2(x, \\theta)) $\n",
    "\n",
    "#### Sample  $u \\sim q(u|x,\\phi) $\n",
    "\n",
    "#### Decoder network\n",
    "\n",
    "$p(\\phi|u,x) \\sim \\mathcal{VM}(\\mu(x,u,\\theta''), \\kappa(x,u,\\theta'')) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CVAE:\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_height=50,\n",
    "                 image_width=50,\n",
    "                 n_channels=3,\n",
    "                 n_hidden_units=8):\n",
    "\n",
    "        self.n_u = n_hidden_units\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.n_channels = n_channels\n",
    "        self.phi_shape = 2\n",
    "\n",
    "        self.x = Input(shape=[self.image_height, self.image_width, self.n_channels])\n",
    "        self.phi = Input(shape=[self.phi_shape])\n",
    "        self.u = Input(shape=[self.n_u])\n",
    "\n",
    "        self.x_vgg = vgg.vgg_model(image_height=self.image_height,\n",
    "                                   image_width=self.image_width)(self.x)\n",
    "\n",
    "        self.x_vgg_shape = self.x_vgg.get_shape().as_list()[1]\n",
    "\n",
    "        self.mu_encoder, self.log_sigma_encoder = self._encoder_mu_log_sigma()\n",
    "        \n",
    "        self.mu_prior, self.log_sigma_prior = self._prior_mu_log_sigma()\n",
    "        \n",
    "        self.u_prior = Lambda(self._sample_u)([self.mu_prior, self.log_sigma_prior])\n",
    "        self.u_encoder = Lambda(self._sample_u)([self.mu_encoder, self.log_sigma_encoder])\n",
    "\n",
    "        self.x_vgg_u = concatenate([self.x_vgg, self.u_encoder])\n",
    "\n",
    "        self.decoder_mu_seq, self.decoder_kappa_seq = self._decoder_net_seq()\n",
    "\n",
    "        self.full_model = Model(inputs=[self.x, self.phi],\n",
    "                                outputs=concatenate([self.mu_encoder,\n",
    "                                                     self.log_sigma_encoder,\n",
    "                                                     self.decoder_mu_seq(self.x_vgg_u),\n",
    "                                                     self.decoder_kappa_seq(self.x_vgg_u)]))\n",
    "\n",
    "        self.decoder_input = concatenate([self.x_vgg, self.u])\n",
    "        self.decoder_model = Model(inputs=[self.x, self.u],\n",
    "                                   outputs=concatenate([self.decoder_mu_seq(self.decoder_input),\n",
    "                                                        self.decoder_kappa_seq(self.decoder_input)]))\n",
    "        \n",
    "    def _encoder_mu_log_sigma(self):\n",
    "\n",
    "        x_vgg_phi = concatenate([self.x_vgg, self.phi])\n",
    "\n",
    "        hidden = Dense(512, activation='relu')(x_vgg_phi)\n",
    "\n",
    "        mu_encoder = Dense(self.n_u, activation='linear')(hidden)\n",
    "        log_sigma_encoder = Dense(self.n_u, activation='linear')(hidden)\n",
    "\n",
    "        return mu_encoder, log_sigma_encoder\n",
    "\n",
    "    def _prior_mu_log_sigma(self):\n",
    "\n",
    "        hidden = Dense(512, activation='relu')(self.x_vgg)\n",
    "\n",
    "        mu_prior = Dense(self.n_u, activation='linear')(hidden)\n",
    "        log_sigma_prior = Dense(self.n_u, activation='linear')(hidden)\n",
    "\n",
    "        return mu_prior, log_sigma_prior\n",
    "    \n",
    "    def _sample_u(self, args):\n",
    "        mu, log_sigma = args\n",
    "        eps = K.random_normal(shape=[self.n_u], mean=0., stddev=1.)\n",
    "        return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "    def _decoder_net_seq(self):\n",
    "        decoder_mu = Sequential()\n",
    "        decoder_mu.add(Dense(512, activation='relu',input_shape=[self.x_vgg_shape + self.n_u]))\n",
    "        # decoder_mu.add(Dense(512, activation='relu'))\n",
    "        decoder_mu.add(Dense(2, activation='linear'))\n",
    "        decoder_mu.add(Lambda(lambda x: K.l2_normalize(x, axis=1)))\n",
    "\n",
    "        decoder_kappa = Sequential()\n",
    "        decoder_kappa.add(Dense(512, activation='relu', input_shape=[self.x_vgg_shape + self.n_u]))\n",
    "        # decoder_kappa.add(Dense(512, activation='relu'))\n",
    "        decoder_kappa.add(Dense(1, activation='linear'))\n",
    "        decoder_kappa.add(Lambda(lambda x: K.abs(x)))\n",
    "        \n",
    "        return decoder_mu, decoder_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_u = 8\n",
    "\n",
    "cvae = CVAE(n_hidden_units=n_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvae_loss(y_true, model_output):\n",
    "    mu_encoder = model_output[:, 0:n_u]\n",
    "    log_sigma_encoder = model_output[:, n_u:n_u*2]\n",
    "    mu_pred = model_output[:, n_u*2:n_u*2+2]\n",
    "    kappa_pred = model_output[:, n_u*2+2:]\n",
    "    log_likelihood = von_mises_log_likelihood_tf(y_true, mu_pred, kappa_pred, input_type='biternion')\n",
    "    kl = 0.5 * K.sum(K.exp(log_sigma_encoder) + K.square(mu_encoder) - 1. - log_sigma_encoder, axis=1)\n",
    "    return K.mean(-log_likelihood + kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvae_elbo_np(y_true, y_pred):\n",
    "    mu_encoder = y_pred[:, 0:n_u]\n",
    "    log_sigma_encoder = y_pred[:, n_u:n_u*2]\n",
    "    mu_pred = y_pred[:, n_u*2:n_u*2+2]\n",
    "    kappa_pred = y_pred[:, n_u*2+2:]\n",
    "    log_likelihood = von_mises_log_likelihood_np(y_true, mu_pred, kappa_pred, input_type='biternion')\n",
    "    kl = 0.5 * np.sum(np.exp(log_sigma_encoder) + np.square(mu_encoder) - 1. - log_sigma_encoder, axis=1).reshape(-1,1)\n",
    "    loss = -log_likelihood+kl\n",
    "    return loss, log_likelihood, kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE()\n",
    "#optimizer = keras.optimizers.Adadelta(lr=0.001)\n",
    "cvae.full_model.compile(optimizer='adam', loss=cvae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model_ckpt_callback = keras.callbacks.ModelCheckpoint('cvae0.h5',\n",
    "                                                      monitor='val_loss',\n",
    "                                                      mode='min',\n",
    "                                                      save_best_only=True,\n",
    "                                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7002 samples, validate on 778 samples\n",
      "Epoch 1/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.6747Epoch 00000: val_loss improved from inf to 1.75168, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 111s - loss: 1.6747 - val_loss: 1.7517\n",
      "Epoch 2/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.4315Epoch 00001: val_loss improved from 1.75168 to 1.62850, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 100s - loss: 1.4315 - val_loss: 1.6285\n",
      "Epoch 3/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.3341Epoch 00002: val_loss improved from 1.62850 to 1.58613, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 100s - loss: 1.3340 - val_loss: 1.5861\n",
      "Epoch 4/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.2199Epoch 00003: val_loss improved from 1.58613 to 1.55113, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 99s - loss: 1.2197 - val_loss: 1.5511\n",
      "Epoch 5/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.2029Epoch 00004: val_loss did not improve\n",
      "7002/7002 [==============================] - 99s - loss: 1.2027 - val_loss: 2.0099\n",
      "Epoch 6/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.1106Epoch 00005: val_loss improved from 1.55113 to 1.30975, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 99s - loss: 1.1105 - val_loss: 1.3097\n",
      "Epoch 7/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.0654Epoch 00006: val_loss did not improve\n",
      "7002/7002 [==============================] - 101s - loss: 1.0652 - val_loss: 1.3129\n",
      "Epoch 8/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.0555Epoch 00007: val_loss did not improve\n",
      "7002/7002 [==============================] - 100s - loss: 1.0556 - val_loss: 1.5935\n",
      "Epoch 9/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 1.0257Epoch 00008: val_loss did not improve\n",
      "7002/7002 [==============================] - 101s - loss: 1.0257 - val_loss: 1.6542\n",
      "Epoch 10/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.9778Epoch 00009: val_loss improved from 1.30975 to 1.29585, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 100s - loss: 0.9779 - val_loss: 1.2959\n",
      "Epoch 11/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.9589Epoch 00010: val_loss did not improve\n",
      "7002/7002 [==============================] - 101s - loss: 0.9589 - val_loss: 1.3949\n",
      "Epoch 12/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.9246Epoch 00011: val_loss improved from 1.29585 to 1.22883, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 102s - loss: 0.9243 - val_loss: 1.2288\n",
      "Epoch 13/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.8490Epoch 00012: val_loss did not improve\n",
      "7002/7002 [==============================] - 101s - loss: 0.8488 - val_loss: 1.3926\n",
      "Epoch 14/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.8226Epoch 00013: val_loss did not improve\n",
      "7002/7002 [==============================] - 101s - loss: 0.8228 - val_loss: 1.2576\n",
      "Epoch 15/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.8088Epoch 00014: val_loss improved from 1.22883 to 1.12424, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 100s - loss: 0.8087 - val_loss: 1.1242\n",
      "Epoch 16/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.7622Epoch 00015: val_loss did not improve\n",
      "7002/7002 [==============================] - 99s - loss: 0.7621 - val_loss: 1.3788\n",
      "Epoch 17/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.7141Epoch 00016: val_loss improved from 1.12424 to 1.08166, saving model to cvae0.h5\n",
      "7002/7002 [==============================] - 99s - loss: 0.7140 - val_loss: 1.0817\n",
      "Epoch 18/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.6813Epoch 00017: val_loss did not improve\n",
      "7002/7002 [==============================] - 98s - loss: 0.6813 - val_loss: 1.4530\n",
      "Epoch 19/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.6507Epoch 00018: val_loss did not improve\n",
      "7002/7002 [==============================] - 98s - loss: 0.6508 - val_loss: 1.8144\n",
      "Epoch 20/20\n",
      "7000/7002 [============================>.] - ETA: 0s - loss: 0.6461Epoch 00019: val_loss did not improve\n",
      "7002/7002 [==============================] - 98s - loss: 0.6460 - val_loss: 1.2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1453accc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.full_model.fit([xtr, ytr_bit], [ytr_bit], batch_size=10, epochs=20, validation_split=0.1,\n",
    "                   callbacks=[model_ckpt_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions using decoder part\n",
    "\n",
    "$ \\phi_i = \\mu(x_i,u_i,\\theta'') $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAAD error (test) : 29.822721 ± 34.245166\n",
      "kappa (test) : 4.475164 ± 3.253803\n",
      "log-likelihood (test) : -0.959275±0.041210SEM\n",
      "ELBO (test) : -0.956658 ± 0.041001 SEM\n",
      "log-likelihood (test) : -0.956260±0.040977SEM\n",
      "KL(encoder|prior) (test) : -0.000398 ± 0.000037 SEM\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "n_samples = xte.shape[0]\n",
    "#ute = np.random.normal(0,1, [n_samples,n_u])\n",
    "ute = np.zeros([n_samples,n_u])\n",
    "\n",
    "#yte_cvae_preds = cvae.full_model.predict([xte, yte_bit])\n",
    "yte_preds = cvae.decoder_model.predict([xte,ute])\n",
    "\n",
    "yte_preds_bit = yte_preds[:,0:2]\n",
    "kappa_preds_te = yte_preds[:,2:]\n",
    "\n",
    "yte_preds_deg = bit2deg(yte_preds_bit)\n",
    "\n",
    "loss_te = maad_from_deg(yte_preds_deg, yte_deg)\n",
    "mean_loss_te = np.mean(loss_te)\n",
    "std_loss_te = np.std(loss_te)\n",
    "\n",
    "print(\"MAAD error (test) : %f ± %f\" % (mean_loss_te, std_loss_te))\n",
    "\n",
    "print(\"kappa (test) : %f ± %f\" % (np.mean(kappa_preds_te), np.std(kappa_preds_te)))\n",
    "\n",
    "\n",
    "log_likelihood_loss_te = von_mises_log_likelihood_np(yte_bit, yte_preds_bit, kappa_preds_te,\n",
    "                                                     input_type='biternion')\n",
    "\n",
    "print(\"log-likelihood (test) : %f±%fSEM\" % (np.mean(log_likelihood_loss_te), sem(log_likelihood_loss_te)))\n",
    "\n",
    "\n",
    "cvae_preds = cvae.full_model.predict([xte, yte_bit])\n",
    "elbo_te, ll_te, kl_te = cvae_elbo_np(yte_bit, cvae_preds)\n",
    "\n",
    "print(\"ELBO (test) : %f ± %f SEM\" % (np.mean(-elbo_te), sem(-elbo_te)))\n",
    "# print(\"log-likelihood (test) : %f ± %f SEM\" % (np.mean(-ll_te), sem(-ll_te)))\n",
    "print(\"log-likelihood (test) : %f±%fSEM\" % (np.mean(ll_te), sem(ll_te)))\n",
    "print(\"KL(encoder|prior) (test) : %f ± %f SEM\" % (np.mean(-kl_te), sem(-kl_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAAD error (train) : 27.974910 ± 33.729157\n",
      "kappa (train) : 4.457736 ± 3.374130\n",
      "log-likelihood (train) : -0.824053±0.010907SEM\n",
      "ELBO (train) : -0.824432 ± 0.010993 SEM\n",
      "log-likelihood (train) : -0.824085±0.010986SEM\n",
      "KL(encoder|prior) (train) : -0.000347 ± 0.000011 SEM\n"
     ]
    }
   ],
   "source": [
    "n_samples = xtr.shape[0]\n",
    "\n",
    "#utr = np.random.normal(0,1, [n_samples,n_u])\n",
    "utr = np.zeros([n_samples,n_u])\n",
    "\n",
    "#ytr_cvae_preds = cvae.full_model.predict([xtr, ytr_bit])\n",
    "ytr_preds = cvae.decoder_model.predict([xtr,utr])\n",
    "\n",
    "ytr_preds_bit = ytr_preds[:,0:2]\n",
    "kappa_preds_tr = ytr_preds[:,2:]\n",
    "\n",
    "ytr_preds_deg = bit2deg(ytr_preds_bit)\n",
    "\n",
    "loss_tr = maad_from_deg(ytr_preds_deg, ytr_deg)\n",
    "mean_loss_tr = np.mean(loss_tr)\n",
    "std_loss_tr = np.std(loss_tr)\n",
    "\n",
    "print(\"MAAD error (train) : %f ± %f\" % (mean_loss_tr, std_loss_tr))\n",
    "\n",
    "print(\"kappa (train) : %f ± %f\" % (np.mean(kappa_preds_tr), np.std(kappa_preds_tr)))\n",
    "\n",
    "\n",
    "log_likelihood_loss_tr = von_mises_log_likelihood_np(ytr_bit, ytr_preds_bit, kappa_preds_tr,\n",
    "                                                     input_type='biternion')\n",
    "\n",
    "print(\"log-likelihood (train) : %f±%fSEM\" % (np.mean(log_likelihood_loss_tr), sem(log_likelihood_loss_tr)))\n",
    "\n",
    "cvae_preds = cvae.full_model.predict([xtr, ytr_bit])\n",
    "elbo_tr, ll_tr, kl_tr = cvae_elbo_np(ytr_bit, cvae_preds)\n",
    "\n",
    "print(\"ELBO (train) : %f ± %f SEM\" % (np.mean(-elbo_tr), sem(-elbo_tr)))\n",
    "# print(\"log-likelihood (trst) : %f ± %f SEM\" % (np.mean(-ll_tr), sem(-ll_tr)))\n",
    "print(\"log-likelihood (train) : %f±%fSEM\" % (np.mean(ll_tr), sem(ll_tr)))\n",
    "print(\"KL(encoder|prior) (train) : %f ± %f SEM\" % (np.mean(-kl_tr), sem(-kl_tr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
